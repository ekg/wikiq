wikiq: a WikiMedia XML data dump to .tsv parser

author: Erik Garrison <erik@hypervolu.me>


overview:

wikiq is written in C using expat.  It is designed to enable researchers to
rapidly extract revision histories (minus text and comments) from impossibly
large XML datasets.


use:

To use, first make sure you have libexpat and libpcrecpp installed, then:

    % make
    % ./wikiq -h  # prints usage
    % 7za e -so hugewikidatadump.xml | ./wikiq >hugewikidatadump.tsv


features:

In addition to parsing WikiMedia XML data dumps into a tab-separated tabular
format, wikiq extracts article diffs and can execute arbitrary Perl-compatible
regular expressions against the additions and deletions which differentiate any
revision from the previous.  Any number of regular expressions may be supplied
on the command line, and may be tagged using the '-n' option.

MD5 checksums are used at runtime for precise detection of reversions.


output:

wikiq generates these fields for each revision:

title, articleid, revid, timestamp, anon, editor, editorid, minor,
text_length, text_entropy, text_md5, reversion, additions_size, deletions_size
.... and additional fields for each regex executed against add/delete diffs

Boolean fields are TRUE/FALSE except in the case of reversion, which is blank
unless the article is a revert to a previous revision, in which case, it
contains the revision ID of the revision which was reverted to.


author: Erik Garrison <erik@hypervolu.me>
